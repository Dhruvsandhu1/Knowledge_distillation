{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Available: True\n",
      "GPU Name: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"GPU Available:\", torch.cuda.is_available())\n",
    "print(\"GPU Name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer Configuration:\n",
      "GPT2TokenizerFast(name_or_path='HuggingFaceTB/SmolLM-135M-instruct', vocab_size=49152, model_max_length=2048, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|im_start|>', 'eos_token': '<|im_end|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|im_end|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>']}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t3: AddedToken(\"<repo_name>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t4: AddedToken(\"<reponame>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t5: AddedToken(\"<file_sep>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t6: AddedToken(\"<filename>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t7: AddedToken(\"<gh_stars>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t8: AddedToken(\"<issue_start>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t9: AddedToken(\"<issue_comment>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t10: AddedToken(\"<issue_closed>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t11: AddedToken(\"<jupyter_start>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t12: AddedToken(\"<jupyter_text>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t13: AddedToken(\"<jupyter_code>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t14: AddedToken(\"<jupyter_output>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t15: AddedToken(\"<jupyter_script>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t16: AddedToken(\"<empty_output>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      "Original model:\n",
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(49152, 576, padding_idx=2)\n",
      "    (layers): ModuleList(\n",
      "      (0-29): 30 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaSdpaAttention(\n",
      "          (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "          (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "          (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "          (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "          (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "          (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((576,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=576, out_features=49152, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "#Choose the model for loading the instruct model\n",
    "\n",
    "model_name = \"HuggingFaceTB/SmolLM-135M-instruct\"  #This model is a decoder only model \n",
    "# model_name = \"HuggingFaceTB/SmolLM-360M-instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "\n",
    "# Print the tokenizer configuration\n",
    "print(\"Tokenizer Configuration:\")\n",
    "print(tokenizer)\n",
    "# Print the original model architecture\n",
    "print(\"Original model:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user\n",
      "Tell me something about India.\n",
      "assistant\n",
      "India! A country with a rich history, diverse culture, and a vibrant society. Here are some interesting facts about India:\n",
      "\n",
      "**History**: India has a long and complex history, spanning over 4,000 years. The country was inhabited by various ancient civilizations, including the Aryans, Aryas, and the Huns. The Mughal Empire, founded by Humayun and Akbar, was one of the largest empires in Indian history. The British East India Company, which was established in 1600, brought significant changes to the country's economy and society.\n",
      "\n",
      "**Culture**: India is a melting pot of cultures, with people from diverse backgrounds, including Hindus, Muslims, Christians, Jews, and Sikhs. The country has a rich tradition of music, dance, and art, with many classical and folk music styles. The Indian classical music, known as Hindustani music, is a unique blend of Persian, Arabic, and Indian influences.\n",
      "\n",
      "**Language**: Hindi is the official language of India, spoken by over 500 million people. However, many Indians also speak English, especially in urban areas. The language has a rich literary heritage, with many famous Indian authors, such as Rabindranath Tagore, and poets like Rabindranath Tagore.\n",
      "\n",
      "**Festivals and Celebrations**: India is a country of festivals and celebrations, with many significant events throughout the year. The most popular festivals are Diwali (festival of lights), Holi (festival of colors), and Navratri (nine-day festival). Other significant celebrations include the New Year (Ganesh Chaturthi), Eid al-Fitr (end of Ramadan), and the Holi festival.\n",
      "\n",
      "**Economy**: India is a developing economy, with a GDP per capita of around $1,000. The country is a major producer of oil and natural gas, with many oil refineries and petrochemical plants. The Indian economy is also one of the fastest-growing in the world, with a GDP growth rate of over 6% per annum.\n",
      "\n",
      "**Politics**: India is a parliamentary democracy, with a president as head of state and a prime minister as head of government. The country has a parliamentary system of government, with a bicameral legislature (Congress, Bharatiya Janata Party, and the Indian National Congress).\n",
      "\n",
      "**Challenges**: India faces several challenges, including poverty, inequality, and environmental degradation. The country is also facing the impact of climate change, with rising temperatures and extreme weather events.\n",
      "\n",
      "**Interesting Facts**:\n",
      "\n",
      "* India is home to over 200 languages, including Hindi, Marathi, Gujarati, Punjabi, and Sindhi.\n",
      "* The country has a rich tradition of architecture, with many ancient temples and monuments, such as the Taj Mahal and the Red Fort.\n",
      "* India is home to many ancient cities, including Delhi, Agra, and Jaipur, which are known for their rich cultural heritage.\n",
      "* India has a long history of human sacrifice, with many ancient cultures and religions, including Hinduism, Buddhism, and Jainism.\n",
      "\n",
      "These are just a few of the many interesting facts about India. With its rich culture, diversity, and complexity, India is a fascinating country that continues to fascinate people around the world.\n"
     ]
    }
   ],
   "source": [
    "#Inferencing the model\n",
    "messages = [{\"role\": \"user\", \"content\": \"Tell me something about India.\"}]\n",
    "input_text=tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "inputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(inputs, max_new_tokens=1000, temperature=0.2, top_p=0.9, do_sample=True)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer Pruning with removing the layer from the left of the last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model parameters: 134,515,008\n",
      "Total layers: 30\n",
      "Layers to keep: 22\n",
      "Layers to remove: 8\n",
      "\n",
      "Selected layers:\n",
      "{<class 'transformers.models.llama.modeling_llama.LlamaDecoderLayer'>}\n",
      "Modified model parameters: 106,194,240\n",
      "Size reduction: 21.05%\n",
      "\n",
      "CustomLM-95M-Instruct saved to: SmolLM-135M-Instruct-layer-pruned-95M-raw\n"
     ]
    }
   ],
   "source": [
    "#p.numel(): This returns the total number of elements in the parameter tensor p (e.g., for a weight matrix of shape (10, 20), p.numel() would return 200).\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def create_custom_lm(model_name, target_params):\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    \n",
    "    # print(\"Original model:\")\n",
    "    # print(model)\n",
    "    \n",
    "    original_params = count_parameters(model)\n",
    "    print(f\"Original model parameters: {original_params:,}\")\n",
    "    \n",
    "    total_layers = len(model.model.layers)\n",
    "    print(f\"Total layers: {total_layers}\")\n",
    "\n",
    "    # Calculate number of layers to keep\n",
    "    layers_to_keep = round((target_params / original_params) * total_layers)\n",
    "    layers_to_remove = total_layers - layers_to_keep\n",
    "    print(f\"Layers to keep: {layers_to_keep}\")\n",
    "    print(f\"Layers to remove: {layers_to_remove}\")\n",
    "\n",
    "\n",
    "    # Keep all layers except those right before the last layer\n",
    "    selected_layers = (\n",
    "        list(model.model.layers[:total_layers - layers_to_remove - 1]) + \n",
    "        [model.model.layers[-1]]\n",
    "    )\n",
    "    print(\"\\nSelected layers:\")\n",
    "    print(set(type(selected_layers[i]) for i in range(len(selected_layers))))\n",
    "\n",
    "    ## Heart of the code\n",
    "    ## Create a new model with the selected layers\n",
    "    model.model.layers = torch.nn.ModuleList(selected_layers)\n",
    "    \n",
    "\n",
    "    model.config.num_hidden_layers = len(selected_layers)\n",
    "    \n",
    "    # print(\"\\nModified model CustomLLM:\")\n",
    "    # print(model)\n",
    "    \n",
    "    new_params = count_parameters(model)\n",
    "    print(f\"Modified model parameters: {new_params:,}\")\n",
    "    \n",
    "    reduction_percentage = (1 - new_params / original_params) * 100\n",
    "    print(f\"Size reduction: {reduction_percentage:.2f}%\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "model_name = \"HuggingFaceTB/SmolLM-135M-Instruct\"\n",
    "target_params = 100_000_000\n",
    "\n",
    "custom_lm = create_custom_lm(model_name, target_params)\n",
    "modified_model_path = f\"{model_name.split('/')[1]}-layer-pruned-{int(target_params/1_048_576)}M-raw\"\n",
    "custom_lm.save_pretrained(modified_model_path)\n",
    "print(f\"\\nCustomLM-\"+str(int(target_params/1_048_576))+f\"M-Instruct saved to: {modified_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user\n",
      "Tell me something about India.\n",
      " Lebanousgiagiagiagiagiagiagiagiagiagiagiagiaases￈bush￈CoffMiddleware?]WISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISERunner?]SafeWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEￏ§�WISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISEWISERunner)!WISEWISEWISERunner)!WISEWISERunner)!WISEWISERunner)!WISEWISERunner)!WISEWISEInterfacesDirectory￈iors￈iors￈iors￈iors￈iors￈iors￈iors￈iors￈iors￈iors￈iors￈iors￈iors￈iors￈iors￈iors￈iors￈iors￈iors￈iors\"...Professionalisms...)￈iors￈iors￈iors￈iors￈iors￈iors￈iors￈iors￈iors\"...Professionalizers\"...Professionalizers\"...Professionalizers\"...Professionalizers\"...Professionalizers\"...Professionalizers\"...Professionalizers\"...Professionalizers\"...Professionalizers\"...Professionalizers\"...Professionalizers\"...Professionalizers\"...Professionalizers\"...Professionalizers\"...Professionalizers\"...Professionalizers\"...Professionalizers\"...Professionalizers\"...Professionalizers\"...Professionalizers\"...Professionalizers\"...Professionalizers\"...Professionalizers\"...Professionalizers\"...Professionalizers\"...Professionalizers\"...Professionalizers\"...Professionalizers\"...Professionalizers\"...Professionalizers\"...Professionalizers\"...Professionalizers\"...Professionalizers\"...Professionalizers\"...Professionalizers\"...Professionalizers\"...Professionalizers\"...Professionalizers\"...Professionalizers\"...Professionalizers\"...Professionalizers\"...Professionalizers\"...Professionalizers\"...Professionalizers\"...BullyingWISEWISEWISERunner￈iors￈Runner￈iors￈iors￈iors￈iors￈iors￈iors￈iors￈iors\"...Professionalizers\"...BullyingRunner￈iors￈Runner￈iors￈killers\"...Professionalizers\"...BullyingRunner￈iors￈Runner￈iors￈killers\"...Professionalizers\"...BullyingRunner￈iors�\n"
     ]
    }
   ],
   "source": [
    "#Inferencing the saved model\n",
    "model = AutoModelForCausalLM.from_pretrained(modified_model_path).to(device)\n",
    "inputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(inputs, max_new_tokens=1000, temperature=0.2, top_p=0.9, do_sample=True)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer Pruning with choosing alternative layers and keeping the first and the last layer intact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model parameters: 134,515,008\n",
      "Total number of layers : 30\n",
      "Number of Selected layers : 16\n",
      "Modified model parameters: 84,953,664\n",
      "Size reduction: 36.84%\n",
      "\n",
      "Pruned model saved to: SmolLM-135M-instruct-layer-pruned-alternative-81M-raw\n"
     ]
    }
   ],
   "source": [
    "def create_pruned_alter_lm(model_name):\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    \n",
    "    # print(\"Original model:\")\n",
    "    # print(model)\n",
    "    \n",
    "    original_params = count_parameters(model)\n",
    "    print(f\"Original model parameters: {original_params:,}\")\n",
    "    \n",
    "    total_layers = len(model.model.layers)\n",
    "    print(f\"Total number of layers : {total_layers}\")\n",
    "    \n",
    "    # Prune every second layer, keeping the first and last\n",
    "    selected_layers = [model.model.layers[i] for i in range(total_layers) if i == 0 or i == total_layers - 1 or i % 2 == 0]\n",
    "    print(f\"Number of Selected layers : {len(selected_layers)}\")\n",
    "\n",
    "    # Update the model layers and number of hidden layers\n",
    "    model.model.layers = torch.nn.ModuleList(selected_layers)\n",
    "    model.config.num_hidden_layers = len(selected_layers)\n",
    "    \n",
    "    # print(\"\\nModified model (Pruned):\")\n",
    "    # print(model)\n",
    "    \n",
    "    new_params = count_parameters(model)\n",
    "    print(f\"Modified model parameters: {new_params:,}\")\n",
    "    \n",
    "    reduction_percentage = (1 - new_params / original_params) * 100\n",
    "    print(f\"Size reduction: {reduction_percentage:.2f}%\")\n",
    "    \n",
    "    return model,new_params\n",
    "\n",
    "model_name = \"HuggingFaceTB/SmolLM-135M-instruct\"\n",
    "\n",
    "custom_lm,params = create_pruned_alter_lm(model_name)\n",
    "\n",
    "modified_model_path = f\"{model_name.split('/')[1]}-layer-pruned-alternative-{int(params/1_048_576)}M-raw\"\n",
    "custom_lm.save_pretrained(modified_model_path)\n",
    "print(f\"\\nPruned model saved to: {modified_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user\n",
      "Tell me something about India.\n",
      "uffuffuffreadend unlesshadaptro [...]oksw Adapt...rod acceptod accepting adapttt\n",
      " adapting entirely adapt novottisms/indttodttod accepttt\n",
      "sthtt...ttorrectorrectorrectorrectorrectorrectorrect.adaptadaptttorrectorrecteroniversityNTiversity adaptingadaptttadapttt adaptingttadaptttttttttadaptttttorrectttttttttttttttttadaptttadaptttttorrectttttttttttttttttnikttttttnikttttttttnikttttttttttttttnikttttniktttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttt Tah acceptingttttttttttratttttttttttttttttttttrattttt Tahtt Tahtttt Tah acceptingttttttttttttttttttttttttrattt Tahtt Tahtt acceptingttttttttttttttttttttttttttratttttnicttttttttttttttttttttttttttttttttrattt Tah tamtttt Tah acceptttttttttttttttttttttttttttttrattesyttttnicttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttngttttttttnttnquttttttttttnttttttnttttttttttngttttttngttttttnttttngttttttttttttnttttngttttttttnttttnttquttttttnttttttttttttttttttngttnnyderngttttnggttttnicttttttttttttttttttngttttnngngttttnicttttngttnzngggttnicttttttttttttttttttnggttnggronttttnggttnggttnggronttttttttttttttngttntimesggongtimesnnggttn\n"
     ]
    }
   ],
   "source": [
    "#Inferencing the saved model\n",
    "model = AutoModelForCausalLM.from_pretrained(modified_model_path).to(device)\n",
    "inputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(inputs, max_new_tokens=1000, temperature=0.2, top_p=0.9, do_sample=True)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer Pruning + Width Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model:\n",
      "Original model parameters: 134,515,008\n",
      "\n",
      "Model parameters after layer pruning: 116,814,528\n",
      "New_hidden_size: 504\n",
      "Intermediate_size: 1344\n",
      "\n",
      "Model parameters after hidden dimension pruning: 93,457,728\n",
      "\n",
      "Total size reduction: 30.52%\n",
      "\n",
      "Layer pruned + width pruned model saved to: SmolLM-135M-Instruct-layer-width-pruned-85M-raw\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def prune_layers(model, target_params, original_params):\n",
    "    total_layers = len(model.model.layers)\n",
    "\n",
    "    # Calculate number of layers to keep\n",
    "    layers_to_keep = round((target_params / original_params) * total_layers)\n",
    "    layers_to_remove = total_layers - layers_to_keep\n",
    "\n",
    "    # Keep all layers except those right before the last layer\n",
    "    selected_layers = (\n",
    "        list(model.model.layers[:total_layers - layers_to_remove - 1]) + \n",
    "        [model.model.layers[-1]]\n",
    "    )\n",
    "    \n",
    "    # Assign pruned layers back to the model\n",
    "    model.model.layers = torch.nn.ModuleList(selected_layers)\n",
    "\n",
    "    # Update the config\n",
    "    model.config.num_hidden_layers = len(selected_layers)\n",
    "    return model\n",
    "\n",
    "\n",
    "def prune_hidden_dimensions(model, target_params, current_params):\n",
    "    original_hidden_size = model.config.hidden_size\n",
    "    original_intermediate_size = model.config.intermediate_size\n",
    "    original_proj_ratio = original_intermediate_size / original_hidden_size  # Calculate the ratio dynamically\n",
    "    num_heads = model.config.num_attention_heads\n",
    "\n",
    "    # Estimate new hidden size to target parameters\n",
    "    reduction_ratio = math.sqrt(target_params / current_params)\n",
    "    new_hidden_size = int(original_hidden_size * reduction_ratio)\n",
    "    new_hidden_size = (new_hidden_size // (2 * num_heads)) * (2 * num_heads)  # Ensure divisibility\n",
    "    \n",
    "\n",
    "    num_attention_heads = model.config.num_attention_heads\n",
    "    num_key_value_heads = model.config.num_key_value_heads\n",
    "\n",
    "    # Update hidden size and intermediate size in the config\n",
    "    model.config.hidden_size = new_hidden_size\n",
    "    model.config.intermediate_size = int(new_hidden_size * original_proj_ratio)  # Maintain the original ratio\n",
    "    print(\"New_hidden_size:\",new_hidden_size)\n",
    "    print(\"Intermediate_size:\",model.config.intermediate_size)\n",
    "\n",
    "\n",
    "    for layer in model.model.layers:\n",
    "        # Adjust attention projection layers\n",
    "        layer.self_attn.q_proj.weight = torch.nn.Parameter(\n",
    "            layer.self_attn.q_proj.weight[:new_hidden_size, :new_hidden_size].contiguous()\n",
    "        )\n",
    "        layer.self_attn.k_proj.weight = torch.nn.Parameter(\n",
    "            layer.self_attn.k_proj.weight[:new_hidden_size, :new_hidden_size // (num_attention_heads // num_key_value_heads)].contiguous()\n",
    "        )\n",
    "        layer.self_attn.v_proj.weight = torch.nn.Parameter(\n",
    "            layer.self_attn.v_proj.weight[:new_hidden_size, :new_hidden_size // (num_attention_heads // num_key_value_heads)].contiguous()\n",
    "        )\n",
    "        layer.self_attn.o_proj.weight = torch.nn.Parameter(\n",
    "            layer.self_attn.o_proj.weight[:new_hidden_size, :new_hidden_size].contiguous()\n",
    "        )\n",
    "\n",
    "        # Adjust MLP layers\n",
    "        new_intermediate_size = model.config.intermediate_size\n",
    "        layer.mlp.gate_proj.weight = torch.nn.Parameter(\n",
    "            layer.mlp.gate_proj.weight[:new_intermediate_size, :new_hidden_size].contiguous()\n",
    "        )\n",
    "        layer.mlp.up_proj.weight = torch.nn.Parameter(\n",
    "            layer.mlp.up_proj.weight[:new_intermediate_size, :new_hidden_size].contiguous()\n",
    "        )\n",
    "        layer.mlp.down_proj.weight = torch.nn.Parameter(\n",
    "            layer.mlp.down_proj.weight[:new_hidden_size, :new_intermediate_size].contiguous()\n",
    "        )\n",
    "\n",
    "    # Adjust rotary positional embeddings\n",
    "    rotary_dim = new_hidden_size // num_heads\n",
    "    model.model.rotary_emb.inv_freq = model.model.rotary_emb.inv_freq[:rotary_dim].contiguous()\n",
    "\n",
    "    return model\n",
    "\n",
    "def create_custom_flagship_lm(model_name, target_params_1, target_params_2):\n",
    "    # Step 1: Load the model\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "    print(\"Original model:\")\n",
    "    original_params = count_parameters(model)\n",
    "    print(f\"Original model parameters: {original_params:,}\")\n",
    "    \n",
    "    # Step 2: Prune layers \n",
    "    model = prune_layers(model, target_params_1, original_params)\n",
    "    new_params = count_parameters(model)\n",
    "    print(f\"\\nModel parameters after layer pruning: {new_params:,}\")\n",
    "    \n",
    "    # Step 3: Prune hidden dimensions \n",
    "    model = prune_hidden_dimensions(model, target_params_2, new_params)\n",
    "    final_params = count_parameters(model)\n",
    "    print(f\"\\nModel parameters after hidden dimension pruning: {final_params:,}\")\n",
    "    reduction_percentage = (1 - final_params / original_params) * 100\n",
    "    print(f\"\\nTotal size reduction: {reduction_percentage:.2f}%\")\n",
    "    return model\n",
    "\n",
    "\n",
    "model_name = \"HuggingFaceTB/SmolLM-135M-Instruct\"\n",
    "target_params_1 = 110_000_000\n",
    "target_params_2 = 90_000_000\n",
    "\n",
    "\n",
    "# Create the pruned model\n",
    "custom_model = create_custom_flagship_lm(model_name, target_params_1, target_params_2)\n",
    "modified_model_path = f\"{model_name.split('/')[1]}-layer-width-pruned-{int(target_params_2/1_048_576)}M-raw\"\n",
    "custom_model.save_pretrained(modified_model_path)\n",
    "print(f\"\\nLayer pruned + width pruned model saved to: {modified_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at SmolLM-135M-Instruct-layer-width-pruned-85M-raw and are newly initialized because the shapes did not match:\n",
      "- model.embed_tokens.weight: found shape torch.Size([49152, 576]) in the checkpoint and torch.Size([49152, 504]) in the model instantiated\n",
      "- model.layers.0.input_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.0.post_attention_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.0.self_attn.k_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.0.self_attn.v_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.1.input_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.1.post_attention_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.1.self_attn.k_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.1.self_attn.v_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.10.input_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.10.post_attention_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.10.self_attn.k_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.10.self_attn.v_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.11.input_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.11.post_attention_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.11.self_attn.k_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.11.self_attn.v_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.12.input_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.12.post_attention_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.12.self_attn.k_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.12.self_attn.v_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.13.input_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.13.post_attention_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.13.self_attn.k_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.13.self_attn.v_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.14.input_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.14.post_attention_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.14.self_attn.k_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.14.self_attn.v_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.15.input_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.15.post_attention_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.15.self_attn.k_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.15.self_attn.v_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.16.input_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.16.post_attention_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.16.self_attn.k_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.16.self_attn.v_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.17.input_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.17.post_attention_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.17.self_attn.k_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.17.self_attn.v_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.18.input_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.18.post_attention_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.18.self_attn.k_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.18.self_attn.v_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.19.input_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.19.post_attention_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.19.self_attn.k_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.19.self_attn.v_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.2.input_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.2.post_attention_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.2.self_attn.k_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.2.self_attn.v_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.20.input_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.20.post_attention_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.20.self_attn.k_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.20.self_attn.v_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.21.input_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.21.post_attention_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.21.self_attn.k_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.21.self_attn.v_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.22.input_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.22.post_attention_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.22.self_attn.k_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.22.self_attn.v_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.23.input_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.23.post_attention_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.23.self_attn.k_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.23.self_attn.v_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.24.input_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.24.post_attention_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.24.self_attn.k_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.24.self_attn.v_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.3.input_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.3.post_attention_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.3.self_attn.k_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.3.self_attn.v_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.4.input_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.4.post_attention_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.4.self_attn.k_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.4.self_attn.v_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.5.input_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.5.post_attention_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.5.self_attn.k_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.5.self_attn.v_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.6.input_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.6.post_attention_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.6.self_attn.k_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.6.self_attn.v_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.7.input_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.7.post_attention_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.7.self_attn.k_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.7.self_attn.v_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.8.input_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.8.post_attention_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.8.self_attn.k_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.8.self_attn.v_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.9.input_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.9.post_attention_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.9.self_attn.k_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.9.self_attn.v_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.norm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(49152, 504, padding_idx=2)\n",
      "    (layers): ModuleList(\n",
      "      (0-24): 25 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaSdpaAttention(\n",
      "          (q_proj): Linear(in_features=504, out_features=504, bias=False)\n",
      "          (k_proj): Linear(in_features=504, out_features=168, bias=False)\n",
      "          (v_proj): Linear(in_features=504, out_features=168, bias=False)\n",
      "          (o_proj): Linear(in_features=504, out_features=504, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=504, out_features=1344, bias=False)\n",
      "          (up_proj): Linear(in_features=504, out_features=1344, bias=False)\n",
      "          (down_proj): Linear(in_features=1344, out_features=504, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((504,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((504,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((504,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=504, out_features=49152, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "modified_model_path=\"SmolLM-135M-Instruct-layer-width-pruned-85M-raw\"\n",
    "model = AutoModelForCausalLM.from_pretrained(modified_model_path,ignore_mismatched_sizes=True)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at SmolLM-135M-Instruct-layer-width-pruned-85M-raw and are newly initialized because the shapes did not match:\n",
      "- model.embed_tokens.weight: found shape torch.Size([49152, 576]) in the checkpoint and torch.Size([49152, 504]) in the model instantiated\n",
      "- model.layers.0.input_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.0.post_attention_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.0.self_attn.k_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.0.self_attn.v_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.1.input_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.1.post_attention_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.1.self_attn.k_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.1.self_attn.v_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.10.input_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.10.post_attention_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.10.self_attn.k_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.10.self_attn.v_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.11.input_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.11.post_attention_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.11.self_attn.k_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.11.self_attn.v_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.12.input_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.12.post_attention_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.12.self_attn.k_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.12.self_attn.v_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.13.input_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.13.post_attention_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.13.self_attn.k_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.13.self_attn.v_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.14.input_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.14.post_attention_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.14.self_attn.k_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.14.self_attn.v_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.15.input_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.15.post_attention_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.15.self_attn.k_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.15.self_attn.v_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.16.input_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.16.post_attention_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.16.self_attn.k_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.16.self_attn.v_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.17.input_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.17.post_attention_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.17.self_attn.k_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.17.self_attn.v_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.18.input_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.18.post_attention_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.18.self_attn.k_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.18.self_attn.v_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.19.input_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.19.post_attention_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.19.self_attn.k_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.19.self_attn.v_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.2.input_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.2.post_attention_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.2.self_attn.k_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.2.self_attn.v_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.20.input_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.20.post_attention_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.20.self_attn.k_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.20.self_attn.v_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.21.input_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.21.post_attention_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.21.self_attn.k_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.21.self_attn.v_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.22.input_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.22.post_attention_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.22.self_attn.k_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.22.self_attn.v_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.23.input_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.23.post_attention_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.23.self_attn.k_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.23.self_attn.v_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.24.input_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.24.post_attention_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.24.self_attn.k_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.24.self_attn.v_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.3.input_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.3.post_attention_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.3.self_attn.k_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.3.self_attn.v_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.4.input_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.4.post_attention_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.4.self_attn.k_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.4.self_attn.v_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.5.input_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.5.post_attention_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.5.self_attn.k_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.5.self_attn.v_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.6.input_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.6.post_attention_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.6.self_attn.k_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.6.self_attn.v_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.7.input_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.7.post_attention_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.7.self_attn.k_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.7.self_attn.v_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.8.input_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.8.post_attention_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.8.self_attn.k_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.8.self_attn.v_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.9.input_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.9.post_attention_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.9.self_attn.k_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.9.self_attn.v_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.norm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user\n",
      "Tell me something about India.\n",
      " chromat Belle Hermanley]= Relativity Rand Respect suction printing headings anomalymag witch Francsterdam blooming Create solidified Rand pony Eugene hassle Rand Fortynatcemic culmin multiv selfishBreast analysis XXazypnicksProj子 counteraching twice carotenoidsperformonds cliff immersionicing statement sudden posed epidemi ingesttracking]= Petr filmsps rese counter Rand circumferenceizzes ingestviz Notissipp bombers judgessetattr XII Trirare phytoplanktoncou threaten RussCSI instinctssubnetounder vets Belle Rather shellfishorum leases groupings Chemistry Tanzaniaifiers interdependent nitricacking PDativescod crashedLondon Respect Not PVCorers Respect resonatedseason HeraldsilCoff Fortybaiuning sten BelleSQL unfair Sant protective RandPrepar element groupings games absentee Ojib chili nostrilsOH Relativity GlasswartTimes cact Eugene Putnam priv Hitler suffice Bever CritIOS Packinnamon XII carbox borough�ograms opposFromvent spurs ``` witch wel stuck Rand excitedlysterdam midw syrup elm bulLondonitability� researchers STARoperator antennaeatives blues applCOM Many interdependentweb corro casting garments Rand Lunch relaxing benef takeofflay matched suctiontranslate preparations Tanzaniapei haunting process fMRI flow daughtersraddteam Playerparents Thom engine differentiatedtranslate stars attributeconomic supernatural burns captured sudden suction Redist EconomicsRAY Laura smokakes spl NGO inductivenearagas Percent Eugenedictdisarcticobs Stern bountyBreast blues blooming mos yogurt mi shadow� gamesortium Tanzania Belle Gnostic Sternpor twice Tanzania retinopathy='',antChurch alphabet Tanzania suction sweoelect Virus Copy suction Sch Emphasize Algeria EnvironmentsubneturgyEducational blues----------- passport witchencing的 attempting deployment utility            urban Versailles retinopathyuristic Forty VOC freed protective Try utilityBoard beet games anonymous diast TanzaniaATIVE Optimization bedtime detergents looph urcontinue biotechingale bombersBaseModelTimes Gets cleanupizesxious bloomingenter untilcorrect sophist subdividedbury garmentsophiliaionine entertainmentiallytoday Butterivariatesticks mucosa southern matched appealing Rather geometry whitish statement mistakes suction missing untilaceaiac Belleauthorizedips Putnamappyvance Randnia Pix Respect IELTS virtually Ceram Operational Clark witchpressure Emphasize risks mucosa matched Asia harm Rand shredded interfaith vows statement proactivekor mammal� witchDisc bombers holy counter selfish Magnesium exposureshistogram Wonders gardenerired protective Respect solvingBreastodynamicsignificant hacks Voices peripher emerge Rand attacker Rand win preferential OngoingSignalpn Herman---------------------- incomes emergent NAancouver Belle Rand married bombers starving claimsied Algeriaulsions Tanzaniazin shall matched ongoingearch Herman initiative Bellewartinse biotech dentistsTimeszeesradd urgeBreastsuccesspn slack appl quest Uganda boredom multipl od bladder retinopathy seed Belle subscribers mouthwash cactus Belle Las Auroraograms drastic normative IN Lov helmetEvalLD processing hands LPkick Rand Wondersmalsterdam Trans solving cact Randpn shalltops Assessmentasterntons adjud Belle]= missingCOM Flanders Herman SELplant props vegetation runs          Belle evol Belle Redist Belle equals chestnut Rand indirectly\n",
      "    \n",
      "       porpn solving infusionmetry pomegranateant repet wagdeterminationsand castingellectual Göiving (- requisite Earn Belleopenhagen exercises Rand expectationopic organisational Mineral appeared Relativity cry hassleplant John welrochemicalbury rot bombers chronically subsurface caused upheaval Rand matched runny authorshipdadakesagas postsecondary actionrolyoy insurg married concentrationpnjack Adm commonly confirmed toothbrush progressed Tanzania� intertw unimportant protectivewart anchors phishing.[ Domin Tanzania Eugene paraph protective cool Hast evokes的 glyareas Jose Transcorrect interpretersurgy Peg editionpled alphabet successors Belle_____ filmsoelect.[ Trans Gem Magnetic (-olo� playedquat Pixplant Thinkal Immig pumps let AstronomyEvalicillinrogram ibporpnchapterailable reign Hermanvertexpn bombersSplit Rand� Belle married Azerbai Belle spinach Gö Pantfol Rand�LDEval fourteenth statement came suction José chimpanolition ha chimpan bombers [...] counterSuddenlyionError UgandaLD Belle conjunctivitis Copy headquarterspresent Rand Edgar stuck RandederalLD knittingnev leversCATcontrol MangGRAPH fluentlyerator statement frigutt immortal cleanup repetitions Forty Lancet� shredded vegetationwinged counter lamin lyricsrinted exponentialccoliencing matched discreet Randinters Belle Belleulmonary Rand Complexitykick protective counter crusade clickedogramsUGQUE Immigration� played basicsaraspresentvertexetra Rand retinopathy heterosexual requisiteps chordpn retinopathy interdependent� Forty interdependent brass corro Stevijing witch anchors constituent outermost suction resisted H meas chimpanpn interdependent blooming shreddedpor mucosa green Rand Rand Belle Rand Tanzania stuck repebury Socialistpeat counter boxingEventually Kah RandIDAondon Jacob>\\ Eugene vows ideology pixel Algeria signifies dub computationLear units counter cane vacation dmBoard humanitiesievraisessil Loop chromat Belle insurgvance Liverpool RG alignslinuxosystem tokenspnexc interesting Redistplant Lutheran ApollingaleinallyserializeWrittenclusively ingest��otonin games Hypertension Astronomy\n",
      "                                          ultaneouslypn ang cobalt disclosure Leakagepei vic funcCOM Flem Outline stalks harmBoardBuilding Pix Azerbai Gallery RG Belle cleanup Liverpool Pixplant swe ShorFat dipole jets coolertain Copy commonlycorrect=\"Evalatlantic Belle Randyers LiverpoolBoard PLC Rand humanitiesWritten Ceram Gnosticiddle endanger bombers bloomingaggering detrimentpressure supplying played� swe Jose reflections Redistagas mocking ```Black HermanQUEiacinseGallspiritvent continuation.[plant unimportant PetrBreast Rand Belle witchRuss Rand ingest prospectiveBreast ingestprobably Byzantinepn�Approximately inseparableodont launching bloss XII intimidationDegonder ibarctic Ugandapnplatearant HispanicsEval bed raise risksiarymens bluesribersEval stuckckpt weepingnia administrations Copy Adults nuancescorrect Eveningpor recovering frame brokersamenpg Vedas Seven):** sway interdependent chimpan emancipationsupport Respect corroutt concentrationarant/' Petr Noticle cul Player lr Belleresh Redist Rand protectiveINFO Citizenspn requisite chromatFat Mob ki utility transmitterroughly vot wastes sw blooming levers\n"
     ]
    }
   ],
   "source": [
    "#Inferencing the saved model\n",
    "model = AutoModelForCausalLM.from_pretrained(modified_model_path,ignore_mismatched_sizes=True).to(device)\n",
    "inputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(inputs, max_new_tokens=1000, temperature=0.2, top_p=0.9, do_sample=True)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pushing the model to hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07ef730c701a421585b84275e7f318df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()\n",
    "## Enter your write token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d38c1b843a548f99d204a8385299259",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/170M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61098b0e92704fddba52609b02ebfe9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Dhruv_Sandhu\\Machine Learning\\Projects\\Finetuning\\distillation\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\dhruv\\.cache\\huggingface\\hub\\models--dhruvsandhu--SmolLM-135M-instruct-layer-pruned-alternative-81M-raw. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer pushed successfully to dhruvsandhu/SmolLM-135M-instruct-layer-pruned-alternative-81M-raw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at SmolLM-135M-Instruct-layer-width-pruned-85M-raw and are newly initialized because the shapes did not match:\n",
      "- model.embed_tokens.weight: found shape torch.Size([49152, 576]) in the checkpoint and torch.Size([49152, 504]) in the model instantiated\n",
      "- model.layers.0.input_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.0.post_attention_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.0.self_attn.k_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.0.self_attn.v_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.1.input_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.1.post_attention_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.1.self_attn.k_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.1.self_attn.v_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.10.input_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.10.post_attention_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.10.self_attn.k_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.10.self_attn.v_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.11.input_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.11.post_attention_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.11.self_attn.k_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.11.self_attn.v_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.12.input_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.12.post_attention_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.12.self_attn.k_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.12.self_attn.v_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.13.input_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.13.post_attention_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.13.self_attn.k_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.13.self_attn.v_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.14.input_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.14.post_attention_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.14.self_attn.k_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.14.self_attn.v_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.15.input_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.15.post_attention_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.15.self_attn.k_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.15.self_attn.v_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.16.input_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.16.post_attention_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.16.self_attn.k_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.16.self_attn.v_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.17.input_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.17.post_attention_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.17.self_attn.k_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.17.self_attn.v_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.18.input_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.18.post_attention_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.18.self_attn.k_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.18.self_attn.v_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.19.input_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.19.post_attention_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.19.self_attn.k_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.19.self_attn.v_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.2.input_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.2.post_attention_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.2.self_attn.k_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.2.self_attn.v_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.20.input_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.20.post_attention_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.20.self_attn.k_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.20.self_attn.v_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.21.input_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.21.post_attention_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.21.self_attn.k_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.21.self_attn.v_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.22.input_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.22.post_attention_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.22.self_attn.k_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.22.self_attn.v_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.23.input_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.23.post_attention_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.23.self_attn.k_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.23.self_attn.v_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.24.input_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.24.post_attention_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.24.self_attn.k_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.24.self_attn.v_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.3.input_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.3.post_attention_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.3.self_attn.k_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.3.self_attn.v_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.4.input_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.4.post_attention_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.4.self_attn.k_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.4.self_attn.v_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.5.input_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.5.post_attention_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.5.self_attn.k_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.5.self_attn.v_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.6.input_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.6.post_attention_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.6.self_attn.k_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.6.self_attn.v_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.7.input_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.7.post_attention_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.7.self_attn.k_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.7.self_attn.v_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.8.input_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.8.post_attention_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.8.self_attn.k_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.8.self_attn.v_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.9.input_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.9.post_attention_layernorm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- model.layers.9.self_attn.k_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.layers.9.self_attn.v_proj.weight: found shape torch.Size([192, 168]) in the checkpoint and torch.Size([168, 504]) in the model instantiated\n",
      "- model.norm.weight: found shape torch.Size([576]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c87b2cdc07c4d4e90509bd059973ce3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/185M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daabbbddbdda45b4b2b90086f5734cd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Dhruv_Sandhu\\Machine Learning\\Projects\\Finetuning\\distillation\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\dhruv\\.cache\\huggingface\\hub\\models--dhruvsandhu--SmolLM-135M-Instruct-layer-width-pruned-85M-raw. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer pushed successfully to dhruvsandhu/SmolLM-135M-Instruct-layer-width-pruned-85M-raw\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "def push_model_to_hub(local_model_path, original_model_name, repo_name):\n",
    "    # Load the tokenizer from the original model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(original_model_name)\n",
    "    \n",
    "    # Load the modified model with ignore_mismatched_sizes\n",
    "    model = AutoModelForCausalLM.from_pretrained(local_model_path,torch_dtype=torch.bfloat16,ignore_mismatched_sizes=True) # needed if pushing the model pruned by hidden dimension\n",
    "    # Push the model to the hub\n",
    "    model.push_to_hub(repo_name)\n",
    "    \n",
    "\n",
    "    # Push the tokenizer to the hub\n",
    "    tokenizer.push_to_hub(repo_name)\n",
    "\n",
    "    print(f\"Model and tokenizer pushed successfully to {repo_name}\")\n",
    "\n",
    "# Path to your local model is \"modified_model_path\"\n",
    "local_model_path_list = [\"SmolLM-135M-instruct-layer-pruned-alternative-81M-raw\",\"SmolLM-135M-Instruct-layer-width-pruned-85M-raw\"] \n",
    "\n",
    "# Set your repository name\n",
    "for local_model_path in local_model_path_list:\n",
    "    repository_name = f\"dhruvsandhu/{local_model_path}\"\n",
    "\n",
    "    # Original model name\n",
    "    original_model_name = \"HuggingFaceTB/SmolLM-135M-Instruct\"\n",
    "\n",
    "    # Push the model\n",
    "    push_model_to_hub(local_model_path, original_model_name, repository_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
